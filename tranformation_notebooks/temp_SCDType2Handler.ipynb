{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79f81546-c7e7-4590-916b-9a76f2c92e8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pyspark.sql.functions import current_timestamp, sha2, concat_ws, lit, col\n",
    "from pyspark.sql.types import TimestampType\n",
    "import uuid\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SCDType2Handler:\n",
    "    \n",
    "    def __init__(self, parameters):\n",
    "        \n",
    "        self.unpack_params(parameters)\n",
    "        self.batch_id = str(uuid.uuid4()) \n",
    "        self.audit_columns =   {\n",
    "            \"__ActivationDateTime\", \"__BusinessKeyHash\", \"__CreateDateTime\", \"__CreatedBatchLogId\", \"__CurrentFlag\",\n",
    "            \"__DeactivationDateTime\", \"__DeletedFlag\", \"__EffectiveEndDateTime\", \"__EffectiveStartDateTime\",\n",
    "            \"__FactKeyHash\", \"__Hash1Type\", \"__Hash2Type\", \"__HashKey\", \"__HashValue\", \"__LastModified\", \"__lastmodified\",\n",
    "            \"__UpdateDateTime\", \"__UpdatedBatchLogId\"\n",
    "        }\n",
    "        self.businessColumnsList   = [col.strip()  for col in self.businessColumns.split(\",\") if  col !=  \"\"]\n",
    "        self.typeIColumnsList = [col.strip()  for col in self.typeIColumns.split(\",\") if  col !=  \"\"]\n",
    "        self.effectiveStartDateTime = datetime.datetime.now()\n",
    "        self.effectiveEndDateTime = self.effectiveStartDateTime - datetime.timedelta(seconds=1)\n",
    "        \n",
    "\n",
    "    def unpack_params(self, params):\n",
    "        \"\"\"\n",
    "        Extract the necessary parameters from the dictionary and store them\n",
    "        as attributes on this class. Raise an error if any required key is missing.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            self.businessColumns = params[\"businessColumns\"]\n",
    "            self.typeIColumns = params[\"typeIColumns\"]\n",
    "            self.tableType = params[\"tableType\"]\n",
    "\n",
    "        except KeyError as missing_key:\n",
    "            raise ValueError(f\"Missing required parameter: {missing_key}\")\n",
    "        \n",
    "        if self.tableType not in (\"Stage\", \"Dim\", \"Fact\"):\n",
    "            raise ValueError(f\"tableType value [{self.tableType!r}] is not in allowed list ['Stage', 'Dim', 'Fact']\")\n",
    "            \n",
    "    \n",
    "    \n",
    "    def refresh_timestamp(self):\n",
    "        \n",
    "        self.effectiveStartDateTime = datetime.datetime.now()\n",
    "        self.effectiveEndDateTime = self.effectiveStartDateTime - datetime.timedelta(seconds=1)\n",
    "        \n",
    "\n",
    "\n",
    "    def add_audit_columns(self, df):\n",
    "        \n",
    "        \n",
    "        if  self.tableType == \"Dim\":\n",
    "            \n",
    "    \n",
    "            Hash2Type = sorted(list({col for col in df.columns}  - self.audit_columns  -  set(self.typeIColumnsList)))\n",
    "            \n",
    "            # Assume df is your source DataFrame and columns like business_key and some_column are present.\n",
    "            df = df.withColumn(\"__CurrentFlag\", lit(True)) \\\n",
    "                .withColumn(\"__DeletedFlag\", lit(False)) \\\n",
    "                .withColumn(\"__EffectiveStartDateTime\",  lit(self.effectiveStartDateTime)  )\\\n",
    "                .withColumn(\"__EffectiveEndDateTime\", lit('2099-12-31').cast(TimestampType()) ) \\\n",
    "                .withColumn(\"__BusinessKeyHash\", sha2(concat_ws(\"|\", *self.businessColumnsList), 256)) \\\n",
    "                .withColumn(\"__Hash1Type\", sha2(concat_ws(\"|\", *self.typeIColumnsList ), 256)) \\\n",
    "                .withColumn(\"__Hash2Type\", sha2(concat_ws(\"|\", *Hash2Type ), 256)) \\\n",
    "                .withColumn(\"__CreatedBatchLogId\", lit(self.batch_id)) \\\n",
    "                .withColumn(\"__CreateDateTime\", current_timestamp()) \\\n",
    "                .withColumn(\"__UpdatedBatchLogId\", lit(None)) \\\n",
    "                .withColumn(\"__UpdateDateTime\", lit(None))\n",
    "               \n",
    "                \n",
    "        elif self.tableType == \"Fact\":\n",
    "            \n",
    "            FactKeyHash = sorted(list({col for col in df.columns}  - self.audit_columns ))\n",
    "            \n",
    "            # Assume df is your source DataFrame and columns like business_key and some_column are present.\n",
    "            df = df.withColumn(\"__DeletedFlag\", lit(False)) \\\n",
    "                .withColumn(\"__FactKeyHash\", sha2(concat_ws(\"|\", *FactKeyHash ), 256)) \\\n",
    "                .withColumn(\"__CreatedBatchLogId\", lit(self.batch_id)) \\\n",
    "                .withColumn(\"__CreateDateTime\", current_timestamp()) \\\n",
    "            \n",
    "        ## silver, stage\n",
    "        else:\n",
    "            \n",
    "            hashValueColumns = sorted(list({col for col in df.columns}  - self.audit_columns  -  set(self.typeIColumnsList)))\n",
    "            \n",
    "            # Assume df is your source DataFrame and columns like business_key and some_column are present.\n",
    "            df = df.withColumn(\"__CurrentFlag\", lit(True)) \\\n",
    "                .withColumn(\"__DeletedFlag\", lit(False)) \\\n",
    "                .withColumn(\"__EffectiveStartDateTime\",  lit(self.effectiveStartDateTime)  )\\\n",
    "                .withColumn(\"__EffectiveEndDateTime\", lit('2099-12-31').cast(TimestampType()) ) \\\n",
    "                .withColumn(\"__lastmodified\", current_timestamp()) \\\n",
    "                .withColumn(\"__HashKey\", sha2(concat_ws(\"|\", *self.businessColumnsList), 256)) \\\n",
    "                .withColumn(\"__HashValue\", sha2(concat_ws(\"|\", *hashValueColumns ), 256)) \\\n",
    "                .withColumn(\"__CreatedBatchLogId\", lit(self.batch_id)) \\\n",
    "                .withColumn(\"__CreateDateTime\", current_timestamp()) \\\n",
    "                .withColumn(\"__UpdatedBatchLogId\", lit(None)) \\\n",
    "                .withColumn(\"__UpdateDateTime\", lit(None))\n",
    "            \n",
    "        return df\n",
    "    \n",
    "\n",
    "    def delta_merge_typeII(self, target_delta_table, source_df):\n",
    "\n",
    "\n",
    "      \n",
    "        \n",
    "        # Load the Silver table as a DeltaTable object\n",
    "\n",
    "        \n",
    " \n",
    "        if  self.tableType == \"Dim\":\n",
    "            \n",
    "            # expire rows  \n",
    "            \n",
    "            if self.typeIColumnsList:\n",
    "            \n",
    "            \n",
    "                update_set = { \n",
    "                        **{column : col(f\"s.{column}\") for column in  self.typeIColumnsList if not column.endswith(\"SID\") },\n",
    "                        \"__UpdatedBatchLogId\" : lit(self.batch_id),\n",
    "                        \"__UpdateDateTime\" : current_timestamp() \n",
    "                        }\n",
    "                \n",
    "             \n",
    "\n",
    "            \n",
    "                ##Type1 Column\n",
    "                target_delta_table.alias(\"t\").merge(\n",
    "                    source = source_df.alias(\"s\"),\n",
    "                    condition = \"\"\" t.__BusinessKeyHash = s.__BusinessKeyHash\n",
    "                                    AND t.__Hash1Type <> s.__Hash1Type\n",
    "                                    AND t.__CurrentFlag = True\n",
    "                                \"\"\"\n",
    "                ).whenMatchedUpdate(\n",
    "                    set = update_set          \n",
    "                ).execute()\n",
    "                \n",
    "            \n",
    "            ##Type2 Column\n",
    "            target_delta_table.alias(\"t\").merge(\n",
    "                source = source_df.alias(\"s\"),\n",
    "                condition = \"\"\" t.__BusinessKeyHash = s.__BusinessKeyHash\n",
    "                                AND t.__Hash2Type <> s.__Hash2Type\n",
    "                                AND t.__CurrentFlag = True\n",
    "                            \"\"\"\n",
    "            ).whenMatchedUpdate(\n",
    "                set = {\n",
    "                    \"__CurrentFlag\": \"false\",\n",
    "                    \"__DeletedFlag\": \"false\",\n",
    "                    \"__UpdatedBatchLogId\" : lit(self.batch_id),\n",
    "                    \"__UpdateDateTime\" : \"current_timestamp()\",\n",
    "                    \"__EffectiveEndDateTime\": f\"cast('{self.effectiveEndDateTime.strftime('%Y-%m-%d %H:%M:%S')}' as timestamp)\",                \n",
    "                }).execute()\n",
    "\n",
    "            target_delta_table.alias(\"t\").merge(\n",
    "                source_df.alias(\"s\"),\n",
    "                condition = \"\"\"\n",
    "                            t.__BusinessKeyHash = s.__BusinessKeyHash\n",
    "                            AND t.__CurrentFlag = True\n",
    "                            \"\"\" \n",
    "            ).whenNotMatchedInsertAll().execute()\n",
    "        \n",
    "        \n",
    "        elif  self.tableType == \"Fact\":\n",
    "            \n",
    "\n",
    "            target_delta_table.alias(\"t\").merge(\n",
    "             source_df.alias(\"s\"),\n",
    "             condition = 't.__FactKeyHash = s.__FactKeyHash'\n",
    "            ).whenNotMatchedInsertAll().execute()\n",
    "               \n",
    "            \n",
    "        else: \n",
    "              \n",
    "            target_delta_table.alias(\"t\").merge(\n",
    "            source = source_df.alias(\"s\"),\n",
    "            condition = \"\"\" t.__HashKey = s.__HashKey\n",
    "                            AND t.__HashValue <> s.__HashValue\n",
    "                            AND t.__CurrentFlag = True\n",
    "                        \"\"\"\n",
    "        ).whenMatchedUpdate(\n",
    "            set = {\n",
    "                \"__CurrentFlag\": \"false\",\n",
    "                \"__DeletedFlag\": \"false\",\n",
    "                \"__UpdatedBatchLogId\" : lit(self.batch_id),\n",
    "                \"__UpdateDateTime\" : \"current_timestamp()\",\n",
    "                \"__EffectiveEndDateTime\": f\"cast('{self.effectiveEndDateTime.strftime('%Y-%m-%d %H:%M:%S')}' as timestamp)\",\n",
    "                \"__lastmodified\": \"current_timestamp()\",\n",
    "            \n",
    "            }).execute()\n",
    "\n",
    "\n",
    "            target_delta_table.alias(\"t\").merge(\n",
    "                source_df.alias(\"s\"),\n",
    "                condition = \"\"\"\n",
    "                            t.__HashKey = s.__HashKey\n",
    "                            AND t.__CurrentFlag = True\n",
    "                            \"\"\" \n",
    "            ).whenNotMatchedInsertAll().execute()\n",
    "        \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "temp_SCDType2Handler",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
