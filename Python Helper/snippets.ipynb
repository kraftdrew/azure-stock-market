{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR StatusLogger Reconfiguration failed: No configuration found for '5ffd2b27' at 'null' in 'null'\n",
      "ERROR StatusLogger Reconfiguration failed: No configuration found for 'Default' at 'null' in 'null'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/PC/Desktop/VS%20Code%20Repositories/azure-stock-market/.venv/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/PC/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/PC/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-70b1a4c2-243c-46e4-812c-71f6874f0fd0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 88ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-70b1a4c2-243c-46e4-812c-71f6874f0fd0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/3ms)\n",
      "25/04/20 18:47:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/20 18:47:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/04/20 18:47:53 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/04/20 18:47:53 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/04/20 18:47:57 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dev_spark_session import DevSparkSession \n",
    "from get_stock_data import GetStockData\n",
    "\n",
    "stockdata = GetStockData() \n",
    "spark = DevSparkSession().spark\n",
    "\n",
    "from pyspark.sql.functions import col,explode,sha2,concat_ws,trim, lit\n",
    "\n",
    "\n",
    "## create table \n",
    "silver_path = \"/Users/PC/Desktop/VS Code Repositories/azure-stock-market/Azure storage/Silver/delta-table\"  # Target location for Silver Delta table\n",
    "\n",
    "\n",
    "spark.sql( f\"\"\"\n",
    "    CREATE TABLE delta_table (\n",
    "        Symbol STRING,\n",
    "        ExchangeName STRING,\n",
    "        Currency STRING,\n",
    "        Type STRING,\n",
    "        ExchangeTimeZone STRING,\n",
    "        Volume STRING,\n",
    "        High STRING,\n",
    "        Low STRING,\n",
    "        Close STRING,\n",
    "        Open STRING,\n",
    "        Date STRING,\n",
    "        __CurrentFlag BOOLEAN,\n",
    "        __DeletedFlag BOOLEAN,\n",
    "        __EffectiveStartDateTime TIMESTAMP,\n",
    "        __EffectiveEndDateTime TIMESTAMP,\n",
    "        __lastmodified TIMESTAMP,\n",
    "        __HashKey STRING,\n",
    "        __HashValue STRING\n",
    ")\n",
    "USING DELTA\n",
    "LOCATION  '{silver_path}';          \n",
    "        \"\"\")\n",
    "\n",
    "\n",
    "# ## re-create table \n",
    "\n",
    "\n",
    "# truncate_schema = spark.read.format(\"delta\").load(silver_path).schema\n",
    "\n",
    "# truncate_silver = spark.createDataFrame([], schema= truncate_schema)\n",
    "# truncate_silver.write.format(\"delta\").mode(\"overwrite\").save(silver_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bronze -> Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from pyspark.sql.types import StructType, StructField, DateType, StringType, DecimalType, IntegerType\n",
    "\n",
    "\n",
    "# schema = StructType([StructField('Symbol', StringType(), True), \n",
    "#             StructField('ExchangeName', StringType(), True), \n",
    "#             StructField('Currency', StringType(), True), \n",
    "#             StructField('Type', StringType(), True), \n",
    "#             StructField('ExchangeTimeZone', StringType(), True), \n",
    "#             StructField('Volume', StringType(), True), \n",
    "#             StructField('High', StringType(), True), \n",
    "#             StructField('Low', StringType(), True), \n",
    "#             StructField('Close', StringType(), True), \n",
    "#             StructField('Open', StringType(), True), \n",
    "#             StructField('Date', StringType(), True)])\n",
    "\n",
    "df_silver = spark.createDataFrame( df_bronze , schema = schema )\n",
    "\n",
    "df_silver.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
