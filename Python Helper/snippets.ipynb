{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR StatusLogger Reconfiguration failed: No configuration found for '5ffd2b27' at 'null' in 'null'\n",
      "ERROR StatusLogger Reconfiguration failed: No configuration found for 'Default' at 'null' in 'null'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/PC/Desktop/VS%20Code%20Repositories/azure-stock-market/.venv/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/PC/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/PC/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-59715ef9-9eb7-4684-b711-74dd9eb2fb1f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 75ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-59715ef9-9eb7-4684-b711-74dd9eb2fb1f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/2ms)\n",
      "25/04/14 20:19:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/14 20:19:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 54551)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/PC/.pyenv/versions/3.11.11/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Users/PC/.pyenv/versions/3.11.11/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Users/PC/.pyenv/versions/3.11.11/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Users/PC/.pyenv/versions/3.11.11/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/PC/Desktop/VS Code Repositories/azure-stock-market/.venv/lib/python3.11/site-packages/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/Users/PC/Desktop/VS Code Repositories/azure-stock-market/.venv/lib/python3.11/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "       ^^^^^^\n",
      "  File \"/Users/PC/Desktop/VS Code Repositories/azure-stock-market/.venv/lib/python3.11/site-packages/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/PC/Desktop/VS Code Repositories/azure-stock-market/.venv/lib/python3.11/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from dev_spark_session import DevSparkSession \n",
    "from get_stock_data import GetStockData\n",
    "\n",
    "stockdata = GetStockData() \n",
    "spark = DevSparkSession().spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,explode,sha2,concat_ws,trim, lit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bronze -> Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/14 20:19:18 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## create table \n",
    "silver_path = \"/Users/PC/Desktop/VS Code Repositories/azure-stock-market/Azure storage/Silver/delta-table\"  # Target location for Silver Delta table\n",
    "\n",
    "\n",
    "# spark.sql( f\"\"\"\n",
    "#     CREATE TABLE my_delta_table (\n",
    "#         Symbol STRING,\n",
    "#         ExchangeName STRING,\n",
    "#         Currency STRING,\n",
    "#         Type STRING,\n",
    "#         ExchangeTimeZone STRING,\n",
    "#         Volume STRING,\n",
    "#         High STRING,\n",
    "#         Low STRING,\n",
    "#         Close STRING,\n",
    "#         Open STRING,\n",
    "#         Date STRING,\n",
    "#         __CurrentFlag BOOLEAN,\n",
    "#         __DeletedFlag BOOLEAN,\n",
    "#         __ActivationDateTime TIMESTAMP,\n",
    "#         __DeactivationDateTime TIMESTAMP,\n",
    "#         __lastmodified TIMESTAMP,\n",
    "#         __HashKey STRING,\n",
    "#         __HashValue STRING\n",
    "# )\n",
    "# USING DELTA\n",
    "# LOCATION  '{silver_path}';          \n",
    "#         \"\"\")\n",
    "\n",
    "\n",
    "## re-create table \n",
    "\n",
    "\n",
    "truncate_schema = spark.read.format(\"delta\").load(silver_path).schema\n",
    "\n",
    "truncate_silver = spark.createDataFrame([], schema= truncate_schema)\n",
    "truncate_silver.write.format(\"delta\").mode(\"overwrite\").save(silver_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'numTargetRowsInserted'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m df_history \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124m    DESCRIBE HISTORY delta.`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msilver_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    LIMIT 1\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m df_history \u001b[38;5;241m=\u001b[39m df_history\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moperationMetrics\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mhead()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      9\u001b[0m audit \u001b[38;5;241m=\u001b[39m   { \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputrows\u001b[39m\u001b[38;5;124m\"\u001b[39m :  df_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumOutputRows\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m---> 10\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minserted\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mdf_history\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnumTargetRowsInserted\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdated\u001b[39m\u001b[38;5;124m\"\u001b[39m: df_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumTargetRowsUpdated\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     12\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeleted\u001b[39m\u001b[38;5;124m\"\u001b[39m: df_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumTargetRowsDeleted\u001b[39m\u001b[38;5;124m\"\u001b[39m] }\n\u001b[1;32m     15\u001b[0m audit \u001b[38;5;241m=\u001b[39m { key : value \u001b[38;5;28;01mfor\u001b[39;00m key, value  \u001b[38;5;129;01min\u001b[39;00m audit\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m }\n\u001b[1;32m     17\u001b[0m audit\n",
      "\u001b[0;31mKeyError\u001b[0m: 'numTargetRowsInserted'"
     ]
    }
   ],
   "source": [
    "df_history = spark.sql(f\"\"\"\n",
    "    DESCRIBE HISTORY delta.`{silver_path}` \n",
    "    LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "df_history = df_history.select(\"operationMetrics\").head()[0]\n",
    "\n",
    "\n",
    "audit =   { \"outputrows\" :  df_history[\"numOutputRows\"],\n",
    "            \"inserted\": df_history[\"numTargetRowsInserted\"],\n",
    "            \"updated\": df_history[\"numTargetRowsUpdated\"],\n",
    "            \"deleted\": df_history[\"numTargetRowsDeleted\"] }\n",
    "        \n",
    "\n",
    "audit = { key : value for key, value  in audit.items() if value != '0' }\n",
    "\n",
    "audit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
